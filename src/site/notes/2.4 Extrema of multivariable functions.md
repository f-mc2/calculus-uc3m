---
{"dg-publish":true,"permalink":"/2-4-extrema-of-multivariable-functions/","created":"2023-08-24T17:04:47.239+02:00","updated":"2023-11-07T22:57:43.034+01:00"}
---

# Local and global extrema

We now turn our attention to the problem of finding local and global extrema for scalar multivariable functions.

>[!defn] Definition: local extrema
>Consider the function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$ and the [[2.1 From 1 to many dimensions#^ceea9b\|interior point]] $\mathbf{x}_{0}\in D$:
>1) it is said that $f$ has a **local maximum** at $\mathbf{x}_{0}$ if there is $\epsilon>0$ such that $f(\mathbf{x}_{0})>f(\mathbf{x})$ for all $\mathbf{x}\in B_{\mathbf{x}_{0},\epsilon}$;
>2) it is said that $f$ has a **local minimum** at $\mathbf{x}_{0}$ if there is $\epsilon>0$ such that $f(\mathbf{x}_{0})<f(\mathbf{x})$ for all $\mathbf{x}\in B_{\mathbf{x}_{0},\epsilon}$
>3) it is said that $f$ has a **local extrema** at $\mathbf{x}_{0}$ if $\mathbf{x}_{0}$ is either a local maximum or a local minimum of $f$.

>[!defn] Definition: global extrema
>Consider the function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$ and $\mathbf{x}_{0}\in D$:
>1) it is said that $f$ has a **global maximum** at $\mathbf{x}_{0}$ if $f(\mathbf{x}_{0})>f(\mathbf{x})$ for all $\mathbf{x}\in D$;
>2) it is said that $f$ has a **global minimum** at $\mathbf{x}_{0}$ if $f(\mathbf{x}_{0})<f(\mathbf{x})$ for all $\mathbf{x}\in D$;
>3) it is said that $f$ has a **local extrema** at $\mathbf{x}_{0}$ if $\mathbf{x}_{0}$ is either a global maximum or a global minimum of $f$.

Understanding the behaviour of arbitrary multivariable scalar functions is incredibly difficult. We need to make regularity assumptions on the type of functions we consider in order to be able to attack the problem. At this purpose, multivariable scalar functions that are continuous are particularly important because they admit global extrema when they are defined on sets that are closed and bounded.

>[!defn] Definition: bounded and closed sets
>The set $A\subseteq \mathbb{R}^{n}$ is called **bounded** if there is $M\in\mathbb{R}$ such that $||\mathbf{x}||\leq M$ for every $\mathbf{x}\in A$. 
>The set $A\subseteq \mathbb{R}^{n}$ is called **closed** if it contains all its boundary points.

>[!important] Theorem: [Extreme value theorem](https://math.stackexchange.com/a/881569/86146)
>Given a closed and bounded set $D\subset\mathbb{R}^{n}$, a continuous function $f\colon D\subset\mathbb{R}^{n}\rightarrow\mathbb{R}$ admits global maximum and global minimum.

>[!attention] Remark: the level sets of continuous functions are closed
>Fixing $c\in\mathbb{R}$, it can be proved that the level set 
>$$
>S=\left\{\mathbf{x}\in\mathbb{R}^{n}\,|\quad g(\mathbf{x})=c \right\}
>$$
>of the continuous function $g\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$ is always closed in $\mathbb{R}^{n}$. 

We will develop the tools necessary to attack the problem of finding local and global extrema for multivariable scalar functions that are continuous on a bounded and closed set of $D\subset\mathbb{R}^{n}$ when the boundary $\partial D$ of $D$ is of a particular type. To be able to discuss more general cases would require a level of sophistication that is not compatible with the little time we have at our disposal for the course. However, if you are interested, a first approach is to read carefully the whole chapter 3 of [M-T](https://www.macmillanlearning.com/college/ca/product/Vector-Calculus/p/1429215089), the whole chapter 16 of [S-E-H](https://www.wiley.com/en-us/Calculus:+One+and+Several+Variables,+10th+Edition-p-9780471698043), and then consult the references and external links in the [Wikipedia page](https://en.wikipedia.org/wiki/Multivariable_calculus) for "Multivariable calculus". 

# Critical points and local extrema for multivariable scalar functions

For a multivariable scalar function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$, it is possible to introduce a notion of derivative that takes into account how $f$ changes along an arbitrary direction in $\mathbb{R}^{n}$ determined by the vector $\mathbf{v}\in\mathbb{R}^{n}$. This notion of derivative is called **directional derivative** and is defined below.

>[!defn] Definition: directional derivative
>Consider the function $f\colon D\subseteq\mathbb{R}^{n}\rightarrow\mathbb{R}$, an interior point $\mathbf{x}_{0}\in D$, and the vector $\mathbf{v}\in\mathbb{R}^{n}$. The **directional derivative** $\mathbf{D}_{\mathbf{v}}f(\mathbf{x}_{0})$ of $f$ at $\mathbf{x}_{0}$ along the direction $\mathbf{v}$ is defined as
>$$
>\mathbf{D}_{\mathbf{v}}f(\mathbf{x}_{0}):=\lim_{t\rightarrow 0}\,\frac{f(\mathbf{x}_{0} + t\mathbf{v}) - f(\mathbf{x}_{0})}{t}=\frac{\mathrm{d} f(\mathbf{x}_{0} + t\mathbf{v})}{\mathrm{d}t}(0) ,
>$$
>where the right-hand side is thought of as a vector function of one variable $t\in\mathbb{R}$. A moment of thought reveals that the directional derivative of $f$ at $\mathbf{x}_{0}$ along the direction $\mathbf{e}_{j}$ of the standard basis coincides with the $j$-th partial derivative of $f$ at $\mathbf{x}_{0}$ as defined [[2.3 Derivatives and Taylor's theorem for multivariable functions#^c8b7dc\|here]]. 

Focusing on the vector function $\gamma\colon D'\subseteq \mathbb{R} \rightarrow \mathbb{R}^{n}$ given by
$$
\gamma(t)=\mathbf{x}_{0} + t\mathbf{v},
$$
where $D'$ is determined by the condition $\gamma(D')\subseteq D$, a direct application of the [[2.3 Derivatives and Taylor's theorem for multivariable functions#^9ceaa7\|chain rule]] for the function $f\circ \gamma$ allows to prove the following proposition that gives a direct link between directional derivatives and the gradient vector.

>[!important] Proposition: directional derivatives and the gradient 
>Let $f\colon D\subseteq\mathbb{R}^{n}\rightarrow \mathbb{R}$ be differentiable in $D$, and $\mathbf{x}_{0}\in D$ be an interior point. Then, it holds 
>$$
> \mathbf{D}_{\mathbf{v}}f(\mathbf{x}_{0})= \mathbf{v}\,\cdot\,\nabla f(\mathbf{x}_{0}) ,
>$$
>where $\cdot$ is the scalar product in $\mathbb{R}^{n}$.

>[!rem] Remark: gradient vector as direction of fastest increase
For a differentiable function $(D,  \mathbb{R}^{n},f, \mathbb{R})$, since the scalar product is maximum when $\mathbf{v}$ is parallel to $\nabla f(\mathbf{x}_{0})$, we conclude that the gradient vector points in the direction along which $f$ increases the fastest.

Motivated by the previous remark, we introduce the notion of **critical point** in analogy with the [[1.6 Derivatives and extrema of scalar functions#^d2fcd0\|one-variable scalar case]], and we state how it is connected to local extrema.

>[!defn] Definition: critical point
>Given the function $(D,\mathbb{R}^{n},f,\mathbb{R})$, an **interior point** $\mathbf{x}_{0}\in D$ is called a **critical point** if either $\mathbf{D}f(\mathbf{x}_{0})=\nabla f(\mathbf{x}_{0})=0$ or it does not exist.

>[!important] Proposition: local extrema and critical points
>If $(D,\mathbb{R}^{n},f, \mathbb{R})$ has a local extremum at $\mathbf{x}_{0}\in D$ then $\mathbf{x}_{0}$ is a **critical point** of $f$.

>[!attention] Remark
>The converse of the previous proposition is [[1.6 Derivatives and extrema of scalar functions#^f4bbab\|not true]].

In the genuinely multivariable case (_i.e._, when $n>1$ in $(D,\mathbb{R}^{n},f,\mathbb{R}))$, it is difficult to establish if a critical point is a local extrema. Intuitively speaking, this is due to the fact that a local extrema $\mathbf{x}_{0}$ has to be an extrema along every direction pointing to it, and this is a very demanding request. For instance, let us consider the function $f\colon \mathbb{R}^{2}\rightarrow\mathbb{R}$ given by
$$
f(x,y)=x^{2} - y^{2} .
$$
It is immediate to check that the partial derivatives are
$$
f_{x}=2x,\qquad f_{y}=-2y,
$$
so that $(0,0)$ is a critical point because $\nabla f(0,0)=(0,0)$. However, if we investigate the behaviour of $f$ along the $x$-axis determined by $y=0$ we find $f(x,0)=x^{2}$ which has a global minimum at $x=0$, while the behaviour of $f$ along the $y$-axis gives back $f(0,y)=-y^{2}$ which has a global maximum at $y=0$. Consequently, $(0,0)$ can not be a local extrema for $f$. 

This situation is very typical for multivariable scalar functions, and, as it is clear from the example above, it applies also to very simple functions.

A possible way to investigate the nature of a critical point is associated with the notion of [[2.3 Derivatives and Taylor's theorem for multivariable functions#^1b4383\|second order partial derivative]]. In particular, multivariable scalar functions that are $C^{2}$ are important because of the following proposition.

>[!important] Proposition 
>Let $f\colon D\subseteq \mathbb{R}^{n}\rightarrow\mathbb{R}$ be $C^{2}$ on $A\subseteq D$, and let $\mathbf{x}_{0}\in A$ be a critical point (which can only mean $\nabla f(\mathbf{x}_{0})=\mathbf{0}$ because all the partial derivatives exist when $f$ is $C^{2}$). If the multivariable scalar function  $Hf_{\mathbf{x}_{0}}\colon\mathbb{R}^{n}\rightarrow\mathbb{R}$ given by
>$$
>Hf_{\mathbf{x}_{0}}(\mathbf{h}):=(h^{1}\,\cdots\, h^{n})\left(\begin{matrix}f_{11}|_{\mathbf{x}_{0}}& \cdots & f_{1n}|_{\mathbf{x}_{0}} \\ & & \\ \vdots & &\vdots \\ & & \\ f_{n1}|_{\mathbf{x}_{0}} & \cdots & f_{nn}|_{\mathbf{x}_{0}}\end{matrix}\right)\,\left(\begin{matrix}h^{1} \\ \\\vdots\\ \\h^{n} \end{matrix}\right)
>$$
>>is strictly positive for all $\mathbf{0}\neq\mathbf{h}\in\mathbb{R}^{n}$ then $\mathbf{x}_{0}$ is a **local minimum**; if it is strictly negative for all $\mathbf{0}\neq\mathbf{h}\in\mathbb{R}^{n}$ then $\mathbf{x}_{0}$ is a **local maximum**; if it takes both positive and negative values and the Hessian matrix $Hf_{\mathbf{x}_{0}}$ is [invertible](https://en.wikipedia.org/wiki/Invertible_matrix) then $\mathbf{x}_{0}$ is a **saddle point**.

# Global extrema: parametrizable boundary


# Global extrema: Lagrange multipliers