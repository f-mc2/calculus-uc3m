---
{"dg-publish":true,"permalink":"/3-1-antiderivatives/","created":"2023-08-24T17:06:23.328+02:00","updated":"2023-11-16T22:18:55.249+01:00"}
---

# Anti-derivatives of scalar functions

The anti-derivative (or primitive) of a scalar function is a sort of algebraic inverse of the derivative.

>[!defn] Definition: anti-derivative function
>Let $([a,b],\mathbb{R},f, \mathbb{R})$ be a continuous function. A continuous function $( [a,b],\mathbb{R},F, \mathbb{R})$ is called an **anti-derivative** (or **primitive**) function for $f$ if it is differentiable in $(a,b)$ and it satisfies
>$$ F'(x)=f(x) $$
>for every $x\in (a,b)$.

>[!rem] Remark
>It can be proved that every continuous function $([a,b],\mathbb{R},f, \mathbb{R})$ admits an anti-derivative. This anti-derivative is not unique, but the difference between any two anti-derivatives is a constant. Specifically, if $([a,b],\mathbb{R},F, \mathbb{R})$ and $([a,b],\mathbb{R},G, \mathbb{R})$ are anti-derivatives of $f$ then there exists $c\in\mathbb{R}$ for which
>$$ G(x)=F(x) +c$$
>for all $x\in (a,b)$.

The act of taking anti-derivatives is depicted as
$$
f\mapsto \; \int f\,\mathrm{d}x ,
$$
where it is implicitly assumed that $x$ is the variable upon which $f$ depends.

Given the intimate link between anti-derivatives and definite integrals we will discuss below, the act of taking the anti-derivative is also referred to as **indefinite integration**.

The operation of taking anti-derivatives satisfies a linearity property that essentially follows from the one we already saw for [[1.3 Derivatives of scalar functions#^5d20ad\|derivatives]].

>[!important] Proposition
>Let $f,g\colon I\subseteq \mathbb{R}\rightarrow \mathbb{R}$ be continuous functions and $a,b\in\mathbb{R}$. Then, it follows
>$$
>\int(a\,f(x) + b\,g(x))\,\mathrm{dx}=a\,\int f(x)\,\mathrm{dx}+ b\,\int g(x)\,\mathrm{dx}.
>$$

>[!exmp] Example
>Consider the function $f\colon \mathbb{R}\rightarrow\mathbb{R}$ given by
>$$
>f(x)= a_{n}x^{n} + \cdots a_{1}x + a_{0} .
>$$
>Recalling the linearity property of the anti-derivative operation and the derivative of a polynomial we immediately get 
>$$
>\int f(x) \,\mathrm{d}x =\frac{a_{n}}{n}x^{n+1}\cdots\frac{a_{1}}{2}x^{2} + a_{0}x + c ,
>$$
>where $c\in \mathbb{R}$ characterizes all possible anti-derivatives of $f$.

We can also exploit other properties of derivatives in order to obtain their counterparts for anti-derivatives, as i's done below.

Let us end this section noting that computing antiderivatives is a craft that requires a lot of practice and a lot of study. There are a lot of different techniques for a lot of different problems, and it is impossible for us to discuss even a little fraction of these methods. However, for those willing to explore this beautiful corner of the mathematical landscape, I suggest starting from chapter 8 of this [book](https://newdev.wileyplus.com/math-and-statistics/salas-calculus-one-and-several-variables-10e-eprof01680/).

## Change of variables

From the [[1.3 Derivatives of scalar functions#^c87d27\|chain rule]] we obtain
$$
 \int f(g(x))\,g'(x)\,\mathrm{d}x = f(g(x)) + c ,
$$
which immediately leads to 
$$
\begin{split}
1) \qquad & \int \frac{f'(x)}{f(x)}\,\mathrm{d}x = \ln(|g(x)|) + c \\ & \\
2) \qquad & \int f'(x) (f(x))^{\alpha}\,\mathrm{d}x = \frac{(f(x))^{\alpha +1}}{\alpha + 1}  + c\qquad (\alpha\neq 1) \\ & \\
3) \qquad & \int \frac{f'(x)}{1 + (f(x))^{2}}\,\mathrm{d}x = \arctan((f(x))  + c  \\ & \\
4) \qquad & \int \frac{f'(x)}{\sqrt{1 - (f(x))^{2}}}\,\mathrm{d}x = \arcsin((f(x))  + c  .
\end{split}
$$

We can also look at this application of the chain rule from a slightly different perspective. Specifically, instead of looking at the composite function $f\circ g$ from scratch, we look at the function $F$ which is an anti-derivative of $f$:
$$
F(x) = \int f(x) \mathrm{d}x \quad \Longleftrightarrow\quad F'(x) =f(x) .
$$
Then, we want to investigate what happens if we apply a change of variable through the function $g$. Clearly, we have 
$$
F(x) \quad \Rightarrow \quad F(x(t))=F(g(t))=(F\circ g)(t),
$$
so that the chain rule implies
$$
(F\circ g)'(t)=(F'\circ g)(t)\;g'(t) = (f\circ g)(t)\;g'(t).
$$
Consequently, since taking the anti-derivative is a kind of inverse of taking the derivative, it also holds

$$
F(x(t))=(F\circ g)(t)=\int (f\circ g)(t) \;g'(t)\mathrm{d}t .
$$

Therefore, we can either take the anti-derivative of $f(x)$ with respect to $x$ directly, or first take the anti-derivative of the composite function $(f\circ g)(t)\,g'(t)$ with respect to the auxiliary variable $t$ and then write the result in terms of $x$ by exploiting $t=g^{-1}(x)$. This procedure is known as the change of variable for anti-derivatives, and it is often written as
$$
\int f(x)\;\mathrm{d}x = \left[\int (f\circ g)(t)\;g'(t)\,\mathrm{d}t\right]_{t=g^{-1}(x)},
$$
where the last change of variable $t\mapsto t=g^{-1}(x)$ for the right-hand-side is often dropped to simplify notation, at the expense of clarity and rigour, unfortunately implicitly considered. The change of variable technique is particularly useful to find anti-derivative, but, unfortunately, there is no general rule to build the change of variable $x=g(t)$. Only experience can help you build your intuition.

>[!exmp] Example
>Let us compute the anti-derivative
>$$
>\int \frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\,\mathrm{d}x .
>$$
>It is clear that the dependence from $x$ is only through the function $1-2x$. Moreover, this function appears either to the power $\frac{3}{2}$ or to the power $\frac{1}{2}$. Therefore, we guess it would be useful to introduce an auxiliary variable of the type $t^{m}=1-2x$, where $m$ takes into account the fact that $1-2x$ appears always eleveted to some power. What specific value of $m$ should we take? Every number that makes all the roots disappear would be a wise choice. For instance, we can take $m=6$ so that 
>$$
>\frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\quad\mapsto\quad \frac{1}{\sqrt[3]{(t^{6})^{2}} - \sqrt{t^{6}}} =\frac{1}{t^{4} - t^{3}},
>$$
>but even $m=12$ would be fine. At this point, we determined the function $t(x)=g^{-1}(x)=\sqrt[6]{1-2x}$ because we defined $t$ in terms of $x$. However, we note that
>$$
>t^{6} = 1-2x \quad \Longrightarrow \quad t^{6} - 1 = -2x\quad\Longrightarrow \quad \frac{1-t^{6}}{2}= x,
>$$
>and  we obtain the change of variable function $x=g(t)=\frac{1-t^{6}}{2}$ with its derivative $g'(t)=-3t^{5}$. Therefore, applying the change of variable formula we get
>$$
>\int \frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\,\mathrm{d}t = \int \frac{-3t^{5}}{t^{4} - t^{3}}\,\mathrm{d}t= -3\int \frac{t^{2}}{t -1}\,\mathrm{d}t .
>$$
>At this point, we note that $(t-1)(t+1)=t^{2} - 1$ so that we can also write
>$$
>\frac{t^{2}}{t -1}=\frac{t^{2}-1 + 1}{t -1}= \frac{t^{2}-1}{t -1} + \frac{1}{t -1}= t-1 + \frac{1}{t -1},
>$$
>and also
>$$
> -3\int \frac{t^{2}}{t -1}\,\mathrm{d}t =  -3\int t-1 + \frac{1}{t -1}\,\mathrm{d}t= -\frac{3}{2}(t-1)^{2} - 3\ln|t-1| +c.
> $$
> Consequently, applying the change of variable $t=g^{-1}(t)=\sqrt[6]{1-2x}$ we obtain
>$$
>\int \frac{1}{\sqrt[3]{(1-2x)^{2}} - \sqrt{1-2x}}\,\mathrm{d}x=  -\frac{3}{2}(\sqrt[6]{1-2x}-1)^{2} - 3\ln|\sqrt[6]{1-2x}-1| +c .
>$$

>[!attention] Remark
>A useful tool to guess the change of variable is to remember that, given $x=g(t)$, it holds
> $$
>\frac{\mathrm{d}x}{\mathrm{d}t}=g'(t)
>$$
>  which we formally write as
> $$
> \mathrm{d}x = g'(t) \mathrm{d}t.
> $$
>  Quite similarly, we also have
> $$
>  \frac{\mathrm{d}t}{\mathrm{d}x}=(g^{-1})'(x)=\frac{1}{g'(g^{-1}(x))} \quad \longrightarrow \quad \mathrm{d}t=(g^{-1})'(x)\mathrm{d}x .
>$$

>[!exmp] Example
>Let us compute the anti-derivative 
>$$
>\int f(x)\,\mathrm{d}x=\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x .
>$$
>It may "feel" natural to set $t=g^{-1}(x)=(x+1)^{2}$ as a reasonable change of variable. It then follows that $x=g(t)=\sqrt{t} -1$ so that $g'(t)=\frac{1}{2\sqrt{t}}$ and 
>$$
>\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x = \int \frac{t}{2\sqrt{1-t}} \,\mathrm{d}t .
>$$
>
>This anti-derivative is not immediate, but can be computed using the procedure known as "integration by part" that is discussed below. However, there is another possibile change of variable that, while looking more complex, simplifies the problem. Specifically, we set
>$$
>t=\sqrt{1-(x+1)^{2}},
>$$
>from which it follows that 
>$$
>1-t^{2} = (x+1)^{2} .
>$$
>Moreover, following what is written in the previous remark, we also have
>$$
>\mathrm{d}t =- \frac{(1+x)}{\sqrt{1- (x+1)^{2}}}\mathrm{d}x ,
>$$
>which means that
>$$
>\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x =\int t^{2}-1 \mathrm{d}t,
>$$
>where, as usual, the change of variable $t\mapsto t(x)$ in the right-hand-side is implicit. The anti-derivative on the right-hand-side is immediate being it the anti-derivative of a polynomial function:
>$$
>\int t^{2} -1 \mathrm{d}t = \frac{t^3}{3} -t,
>$$
>and thus we obtain
>$$
>\int\frac{(x+1)^{3}}{\sqrt{1-(x+1)^{2}}}\,\mathrm{d}x = \frac{(1-(x+1)^{2})^{\frac{3}{2}}}{3} - \sqrt{1-(x+1)^{2}}.
>$$

>[!rem] Remark
>It is useful to remember the following useful changes of variables:
>
>1) when we have $\sqrt{1-x^{2}}$ we can apply $x=g(t)=\sin(t)$ so that $\sqrt{1-x^2}=\cos(t)$ and $g'(t)=\cos(t)$;
>2)  when we have $\sqrt{1+x^{2}}$ we can apply $x=g(t)=\tan(t)$ so that $\sqrt{1+x^2}=\frac{1}{\cos(t)}$ and $g'(t)=\frac{1}{\cos^{2}(t)}$;
>3) when we have $\sqrt{1+x^{2}}$ we can also apply $x=g(t)=\sinh(t)$ so that $\sqrt{1+x^2}=\cosh(t)$ and $g'(t)=\cosh(t)$;
>4) when we have $\sqrt{x^{2}-1}$ we can apply $x=g(t)=\frac{1}{\cos(t)}$ so that $\sqrt{x^{2} -1}=\tan(t)$ and $g'(t)=\frac{\sin(t)}{\cos^{2}(t)}$;
>5) when we have $\sqrt{x^{2}-1}$ we can apply $x=g(t)=\cosh(t)$ so that $\sqrt{x^2 - 1}=\sinh(t)$ and $g'(t)=\sinh(t)$;
>6) when we have rational functions of trigonometric functions, we can exploit $t=g^{-1}(x)=\tan(x/2)$ so that $\sin(x)=\frac{2t}{1+t^{2}}$, $\cos(x)=\frac{1-t^{2}}{1+t^{2}}$, and $g'(t)=\frac{2}{1 + t^{2}}$.
>

## Integration by parts

If we exploit the so-called [[1.3 Derivatives of scalar functions#^5d20ad\|Leibniz rule]] for computing the derivative of the product function
$$
\frac{\mathrm{d}}{\mathrm{d}x} (gh)(x)= g'(x) h(x) + g(x) h'(x),
$$
we obtain
$$
\int\left(\frac{\mathrm{d}}{\mathrm{d}x} (gh)(x)\right)\,\mathrm{d}x = \int\left( g'(x) h(x) + g(x) h'(x)\right)\,\mathrm{d}x
$$
which means
$$
\int  g'(x) h(x) \,\mathrm{d}x= gh(x) - \int  g(x) h'(x) \,\mathrm{d}x ,
$$
which is known as the formula for **integration by parts**.

>[!exmp] Example
> Consider the continuous function $f\colon [a,b]\rightarrow \mathbb{R}$ given by $f(x)=x\mathrm{e}^{x}$. We compute its anti-derivatives exploiting the rule of integration by parts. Specifically, we look at $\mathrm{e}^{x}$ as the derivative function of $g(x)=\mathrm{e}^{x}$ so that 
> 
> $$
> \int f(x)\,\mathrm{d}x=\int x\,\mathrm{e}^{x}\,\mathrm{d}x = x \mathrm{e}^{x} - \int \mathrm{e}^{x} \,\mathrm{d}x=(x-1)\mathrm{e}^{x} .
>$$
>
>A direct computation of the derivative of $(x-1)\mathrm{e}^{x}$ shows that we did good.

# Definite integrals and anti-derivatives

Consider the scalar function $f\colon D\subseteq\mathbb{R}\rightarrow\mathbb{R}$ and the closed and bounded interval $[a,b]\subseteq D$, and build the set
$$
S_{[a,b]}^{f}:=\left\{(x,y)\in\mathbb{R}^{2}\,|\quad x\in[a,b],\;\;y=f(x)\right\} .
$$
The notion of definite integral of $f$ over $[a,b]$ gives rigorous meaning to the idea of measuring the area $\mathrm{A}(S_{[a,b]}^{f})$ of $S_{[a,b]}^{f}$ in such a way that this mathematical construction reduces to the results obtained by elementary geometry in simple cases like the area of a square (_e.g._, $f\colon \mathbb{R}\rightarrow \mathbb{R}$ given by $f(x)=1$  over the interval $[0,1]$), or the area of a triangle (_e.g._, $f(x)=x$ over the interval $[0,1]$). 

Let us start with a function $f\colon D\subseteq\mathbb{R}\rightarrow \mathbb{R}$ which is non-negative and increasing. The main idea behind the procedure of integrating $f$ is to approximate the area of $\mathrm{A}(S_{[a,b]}^{f})$ "from above" and "from below". 

To better understand the idea, we visualize the procedure of finding the "approximation from below", on the left, and "from above", on the right ([source-left](https://en.wikipedia.org/wiki/File:Riemann_Integration_and_Darboux_Lower_Sums.gif), [source-right](https://en.wikipedia.org/wiki/File:Riemann_Integration_and_Darboux_Upper_Sums.gif)): 

![integral_lower_upper_sum.gif](/img/user/img/integral_lower_upper_sum.gif)

More mathematically, we first divide $[a,b]$ in two intervals selecting $c\in(a,b)$. The approximation of $\mathrm{A}(S_{[a,b]}^{f})$ from below will be given by
$$
\mathrm{A}_{2}^{-}(S_{[a,b]}^{f})=(c-a)(f(a)) + (b-c)(f(c)).
$$
This is nothing but the sum of the area of the rectangle with vertices $(a,0)$, $(c,0)$, $(a,f(a))$, $(c,f(a))$ with the area of the rectangle with vertices $(c,0)$, $(b,0)$, $(c,f(c))$, $(b,f(c))$. Quite similarly, the approximation of $\mathrm{A}(S_{[a,b]}^{f})$ from above will be given by
$$
\mathrm{A}_{2}^{+}(S_{[a,b]}^{f})=(c-a)(f(c)) + (b-c)(f(b)).
$$

We go on and divide $[a,b]$ in 3 parts and compute $\mathrm{A}_{3}^{-}(S_{[a,b]}^{f})$ and $\mathrm{A}_{3}^{+}(S_{[a,b]}^{f})$, and we go on increasing the number of parts in which we divide $[a,b]$. If the function $f$ is "nice enough", this procedure will give us two sequences, say $\mathrm{A}_{n}^{-}(S_{[a,b]}^{f})$ and $\mathrm{A}_{n}^{+}(S_{[a,b]}^{f})$, that converge to the same limit which is the area $\mathrm{A}(S_{[a,b]}^{f})$ we were looking for. 

The procedure outlined above [can be generalized](https://en.wikipedia.org/wiki/Darboux_integral) to functions which are not necessarily non-negative, and not necessarily increasing, and all functions for which $\mathrm{A}_{a}^{b}(f)$ actually exists are called (Darboux or Riemann) **integrable**. Of course the interpretation of the integral as an area fails for functions admitting negative values, unless we allow for a more flexible idea of areas which contemplates also negative values.

For reasons that will be clear below, the integral $\mathrm{A}(S_{[a,b]}^{f})$ of $f\colon D\subseteq\mathbb{R}\rightarrow\mathbb{R}$ over $[a,b]$ is denoted as
$$
\mathrm{A}(S_{[a,b]}^{f})=\int_{a}^{b} f(x)\,\mathrm{d}x,
$$
where it is clear that we are borrowing the notation from the anti-derivative operation. 

The existence of the integral of a function is not guaranteed in general.

>[!important] Proposition: continuous functions are integrable
>Let $f\colon [a,b]\subset \mathbb{R}\rightarrow\mathbb{R}$ be continuos except at most at $a$ or $b$, then the integral $\mathrm{A}(S_{[a,b]}^{f})$ exists. 

As you might guess, evaluating the integral of a function by means of the approximation procedures described above is incredibly difficult, even for "nice" functions. However, some talented humans discovered a deep, beautiful, and perhaps surprising link between integrals and anti-derivatives that greatly simplify the task.

>[!important] Theorem: [first fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#First_part)
>Let $f\colon [a,b]\rightarrow \mathbb{R}$ be continuous. For every $x\in [a,b]$, the function 
>$$
>F(x):=\int_{a}^{x}f(t)\,\mathrm{d}t =\mathrm{A}(S_{[a,x]}^{f})
>$$
>is continuous on $[a,b]$ and differentiable in $(a,b)$. Moreover, its derivative satisfies
>$$
>F'(x) =f(x)
>$$
>which means that $F(x)$ is an anti-derivative of $f(x)$.

>[!important] Theorem: [second fundamental theorem of calculus](https://en.wikipedia.org/wiki/Fundamental_theorem_of_calculus#Second_part)
>Let $f\colon [a,b]\rightarrow\mathbb{R}$ be such that it admits a continuous anti-derivative $F$ at all $x\in (a,b)$. Then, if $f$ is integrable it holds
>$$
>\mathrm{A}(S_{[a,b]}^{f})=\int_{a}^{b}f(t)\,\mathrm{d}t=F(b) - F(a) .
>$$

Even if the two theorems above are similar, they are different. For instance, the first one assumes $f$ to be continuous so that its anti-derivative is a [[1.3 Derivatives of scalar functions#^c07f2b\|$C^{1}$-function]], while the second one only assumes $F$ to be differentiable. Then, the first one determines the anti-derivative $F$ uniquely since
$$
F(a)=\int_{a}^{a}f(t)\,\mathrm{d}t = \mathrm{A}(S_{[a,a]}^{f}) =0
$$
because the area over a single point is $0$. However, we know that $G(x)=F(x) + c$ with $c\in\mathbb{R}$ is another good anti-derivative. Indeed, $G(x)$ can be used to compute the integral of $f$ using the second theorem since
$$
F(b)\overbrace{=}^{\mathrm{Thm}\,1}\mathrm{A}_{a}^{b}(f)=\int_{a}^{b}f(t)\mathrm{d}t\overbrace{=}^{\mathrm{Thm}\,2}G(b) - G(a)\overbrace{=}^{G(x)=F(x)+c} F(b) +c - F(a) - c \overbrace{=}^{F(a)=0} F(b) .
$$

In the following, when computing $\mathrm{A}(S_{[a,b]}^{f})$ we simply look for an anti-derivative $F$ of $f$ and then write $\mathrm{A}(S_{[a,b]}^{f})=F(b)-F(a)$.

We list now some important results concerning definite integral.

>[!important] Proposition: properties of the integral
>Let $f,g\colon[a,b]\subset\mathbb{R}\rightarrow\mathbb{R}$ be integrable, and let $\alpha,\beta\in\mathbb{R}$. It  holds that
>$$
>\begin{split}
>\int_{c}^{c}f(t)\,\mathrm{d}t&=0 \\ & \\
>\int_{c}^{d}f(t)\,\mathrm{d}t&= - \int_{d}^{c}f(t)\,\mathrm{d}t \\ & \\ \int_{a}^{b}\left(\alpha f(t) + \beta g(t)\right) \,\mathrm{d}t &= \alpha \int_{a}^{b}f(t) \,\mathrm{d}t+ \beta \int_{a}^{b}g(t) \,\mathrm{d}t ,
>\end{split}
>$$
>where the last property is known as the **linearity of the integral**. 

>[!important] Proposition: interval additivity
>Let $a<b<c$ be real numbers. The function $f\colon [a,c]\rightarrow\mathbb{R}$ is integrable on $[a,c]$ if and only if it is integrable in $[a,b]$ and $[b,c]$. Moreover, it holds 
>$$
>\int_{a}^{c}f\;\mathrm{d}x = \int_{a}^{b}f\;\mathrm{d}x + \int_{b}^{c}f\;\mathrm{d}x .
>$$
>In particular, a function $f\colon [a,b]\rightarrow \mathbb{R}$ which is continuous except at a finite (but perhaps arbitrarily big) number of points in $[a,b]$ is integrable in $[a,b]$.

>[!important] Proposition 
>Let $f\colon[a,b]\rightarrow\mathbb{R}$ be continuous, and let $g_{1},g_{2}\colon(a,b)\rightarrow$ be differentiable. Then
>$$
>H(x):=\int_{g_{1}(x)}^{g_{2}(x)}f(t)\;\mathrm{d}t
>$$
>is differentiable in $(a,b)$ and it is such that
>$$
>H'(x)=f(g_{2}(x))\,g_{2}'(x) - f(g_{1}(x))\,g_{1}'(x)
>$$
>for all $x\in (a,b)$.

>[!important] Proposition (Change of variables for definite integrals)
>Let $g\colon[a,b]\rightarrow \mathbb{R}$ be continuous and differentiable in $(a,b)$, and let $f\colon g(a,b)\subset\mathbb{R}\rightarrow \mathbb{R}$ be continuous. Then
>$$
>\int_{g(a)}^{g(b)}\,f(t)\;\mathrm{d}t= \int_{a}^{b}\,f\circ g(x)\;\mathrm{d}x .
>$$

